{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f842ab45",
   "metadata": {},
   "source": [
    "# Keyword Extraction Using LDA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "089c4b02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall topics for category \u001b[1m 'food'\u001b[0m:\n",
      "Topic 1: chicken\n",
      "Topic 2: islamabad\n",
      "Topic 3: burger\n",
      "Topic 4: order\n",
      "Topic 5: islamabadfoodblog\n",
      "Topic 6: foodie\n",
      "Topic 7: biryani\n",
      "Topic 8: cake\n",
      "\n",
      "Overall topics for category \u001b[1m 'GiftShop'\u001b[0m:\n",
      "Topic 1: gift\n",
      "Topic 2: pakistan\n",
      "Topic 3: one\n",
      "Topic 4: day\n",
      "Topic 5: onlineshopping\n",
      "Topic 6: world\n",
      "Topic 7: eid\n",
      "Topic 8: customized\n",
      "\n",
      "Overall topics for category \u001b[1m 'Clothing'\u001b[0m:\n",
      "Topic 1: winter\n",
      "Topic 2: online\n",
      "Topic 3: instores\n",
      "Topic 4: day\n",
      "Topic 5: style\n",
      "Topic 6: fashion\n",
      "Topic 7: elegance\n",
      "Topic 8: adorned\n",
      "\n",
      "Overall topics for category \u001b[1m 'Beauty'\u001b[0m:\n",
      "Topic 1: makeup\n",
      "Topic 2: makeupartist\n",
      "Topic 3: skin\n",
      "Topic 4: beauty\n",
      "Topic 5: look\n",
      "Topic 6: stunning\n",
      "Topic 7: lash\n",
      "Topic 8: winteroutfit\n",
      "\n",
      "Overall topics for category \u001b[1m 'Fitness'\u001b[0m:\n",
      "Topic 1: fitness\n",
      "Topic 2: workout\n",
      "Topic 3: exercise\n",
      "Topic 4: sadiaariffitness\n",
      "Topic 5: winteroutfit\n",
      "Topic 6: outfitideas\n",
      "Topic 7: classy\n",
      "Topic 8: competition\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*****************************************************************#\n",
    "#Cleaning the Data ----Preprocessing\n",
    "#*****************************************************************#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#LDA Module which can Resolve the Error \n",
    "#import gensim \n",
    "#from gensim import corpora\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the Excel file into a pandas dataframe\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "#print(df)\n",
    "# Remove irrelevant columns\n",
    "df = df[['category', 'username', 'captions', 'hashtags']]\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "df['captions'] = df['captions'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Define the pattern to match numeric values\n",
    "pattern = r'\\d+'\n",
    "\n",
    "# Remove numeric values from captions column\n",
    "df['captions'] = df['captions'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Remove numeric values from hashtags column\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['captions'] = df['captions'].apply(lambda x: x.lower())\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "#print(df)\n",
    "# Lemmatize the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Create a corpus for each page separately\n",
    "corpora = []\n",
    "pages = df['username'].unique()\n",
    "for page in pages:\n",
    "    page_df = df[df['username'] == page]\n",
    "    page_corpus = page_df['captions'] + ' ' + page_df['hashtags']\n",
    "    corpora.append(page_corpus)\n",
    "\n",
    "#WordtoVec vector representation\n",
    "#Clustering technique \n",
    "\n",
    "#*****************************************************************#\n",
    "#Performing Topic Modelling ---LDA to get the keywords per page \n",
    "#*****************************************************************#\n",
    "    \n",
    "# Apply LDA per page\n",
    "num_topics =  8 # Specify the number of topics to extract for each page\n",
    "keyword_results = []\n",
    "for corpus in corpora:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
    "    lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "    # print(lda_matrix)\n",
    "    top_keywords = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        # print(lda_model.components_)\n",
    "        # print(\"--------------------Topicidx\")\n",
    "        # print(topic_idx)\n",
    "        topic_keywords = [feature_names[i] for i in topic.argsort()[:-6:-1]]  # Get top 5 keywords\n",
    "        top_keywords.append(topic_keywords)\n",
    "        # top_keywords.append((topic_keywords, lda_model.transform(tfidf_matrix)))\n",
    "\n",
    "    keyword_results.append(top_keywords)\n",
    "# print(lda_model.components_)\n",
    "# print (keyword_results)\n",
    "\n",
    "#*****************************************************************#\n",
    "#Performing Topic Modelling ---to get the keywords per Category \n",
    "#*****************************************************************#\n",
    "    \n",
    "category_keywords = {}\n",
    "for i, page in enumerate(pages):\n",
    "    category = df.loc[df['username'] == page, 'category'].iloc[0]\n",
    "    if category not in category_keywords:\n",
    "        category_keywords[category] = []\n",
    "    category_keywords[category].extend(keyword_results[i])\n",
    "\n",
    "# Flatten the keywords list\n",
    "category_keywords_flat = {category: [keyword for sublist in keywords for keyword in sublist]\n",
    "                          for category, keywords in category_keywords.items()}\n",
    "\n",
    "# Perform second iteration of topic modeling for each category\n",
    "category_topics = {}\n",
    "for category, keywords in category_keywords_flat.items():\n",
    "    keyword_counts = Counter(keywords)\n",
    "    top_keywords = keyword_counts.most_common(num_topics)\n",
    "    category_topics[category] = [keyword for keyword, count in top_keywords]\n",
    "\n",
    "# Print the overall topics for each category\n",
    "for category, topics in category_topics.items():\n",
    "    print(f\"\\nOverall topics for category \\033[1m '{category}'\\033[0m:\")\n",
    "    for i, topic in enumerate(topics):\n",
    "        #print(f\"Topic {i+1}: {', '.join(topic)}\")\n",
    "        print(f\"Topic {i+1}:\",topic)\n",
    "        #print(topic)\n",
    "    #print()\n",
    "\n",
    "\n",
    "# Calculate the score of each keyword\n",
    "category_keyword_scores = {}\n",
    "keyword_frequency_dict={}\n",
    "for category, keywords in category_topics.items():\n",
    "    keyword_scores = {}\n",
    "    sum_keyword_scores = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_score = 0\n",
    "        for i, page_corpus in enumerate(corpora):\n",
    "            # Calculate the frequency of the keyword in the page corpus\n",
    "            keyword_frequency = page_corpus.str.count(keyword).sum()\n",
    "            #print(pages)\n",
    "            #print(keyword_frequency)\n",
    "\n",
    "             # Add the keyword frequency to the keyword frequency dictionary\n",
    "            page_name = pages[i]\n",
    "            if page_name not in keyword_frequency_dict:\n",
    "                keyword_frequency_dict[page_name] = {}\n",
    "            keyword_frequency_dict[page_name][keyword] = keyword_frequency\n",
    "            #print(keyword_frequency_dict[page_name])\n",
    "            # Calculate the total number of words in the page corpus\n",
    "            total_words = len(' '.join(page_corpus.tolist()).split())\n",
    "            # Calculate the frequency of the keyword normalized by the total number of words\n",
    "            #normalized_frequency = keyword_frequency / total_words\n",
    "            #print(lda_matrix.shape[0])\n",
    "            # Add the normalized frequency of the keyword to the keyword score for the page\n",
    "            #keyword_score += normalized_frequency * lda_matrix[i][np.argmax(lda_matrix[i])]\n",
    "        # Calculate the average keyword score across all pages\n",
    "        #keyword_score /= len(corpora)\n",
    "        #keyword_scores[keyword] = keyword_score\n",
    "        #print()\n",
    "        #sum_keyword_scores += keyword_score\n",
    "    # Normalize the score of each keyword\n",
    "    #for keyword in keyword_scores:\n",
    "        #keyword_scores[keyword] /= sum_keyword_scores\n",
    "    #category_keyword_scores[category] = keyword_scores\n",
    "\n",
    "# Print the scores of the top keywords for each category\n",
    "#for category, keyword_scores in category_keyword_scores.items():\n",
    "    #print(f\"Scores of the top keywords for category '{category}':\")\n",
    "    #for i, (keyword, score) in enumerate(sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:num_topics]):\n",
    "        #print(f\"Keyword {i+1}: '{keyword}', Score: {score:.3f}\")\n",
    "    #print()\n",
    "    \n",
    "    \n",
    "#print(\"---------------------------\")\n",
    "#print(category_topics.items())    \n",
    "#print(\"---------------------------\")\n",
    "#print(\"---------------------------\")\n",
    "\n",
    "#for i in pages:\n",
    "    #print(keyword_frequency_dict[i])\n",
    "    \n",
    "#print(\"---------------------------Category Topics-------------------------\")\n",
    "   \n",
    "#print(category_topics)    \n",
    "\n",
    "#print(\"---------------------------Pages/keywords/Frequency-------------------------\")\n",
    "\n",
    "#print(keyword_frequency_dict.items())    \n",
    "\n",
    "\n",
    "#*****************************************************************#\n",
    "#Creating Excel FIle for passing the data to our Learning Model\n",
    "#*****************************************************************#\n",
    "\n",
    "# create an empty list to store the data\n",
    "data_list = []\n",
    "\n",
    "# loop through the pages and keywords to extract the data\n",
    "for page_name, keyword_frequency_dict in keyword_frequency_dict.items():\n",
    "    # find the category for the current page\n",
    "    category = None\n",
    "    for category_name, pages in category_topics.items():\n",
    "        #print(\"***********************\")\n",
    "        #print(category_name)\n",
    "        #print(pages)\n",
    "        #if page_name in pages:\n",
    "            category = df.loc[df['username'] == page_name, 'category'].iloc[0]   \n",
    "            #break\n",
    "    \n",
    "    # extract the keywords and frequencies for the page\n",
    "    keywords = [keyword for keyword, _ in keyword_frequency_dict.items()]\n",
    "    frequencies = [frequency for _, frequency in keyword_frequency_dict.items()]\n",
    "    \n",
    "    # append the data to the list\n",
    "    data_list.append((page_name, *frequencies,category))\n",
    "\n",
    "# create a dataframe from the list\n",
    "df = pd.DataFrame(data_list, columns=[\"username\", *keywords,\"category\"])\n",
    "\n",
    "# write the dataframe to an excel file\n",
    "df.to_excel(\"trainingset.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14933775",
   "metadata": {},
   "source": [
    "# Cleaning of Testing File and Creating Testing File for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01b85c7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Excel File Created for Testing the Model to pass in the Model to Test\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load the Excel file into a pandas dataframe\n",
    "df = pd.read_excel('testingset.xlsx')\n",
    "\n",
    "# Remove irrelevant columns\n",
    "df = df[['username', 'captions', 'hashtags']]\n",
    "\n",
    "# Handle missing values by replacing NaN with an empty string\n",
    "df['captions'] = df['captions'].fillna('').astype(str)\n",
    "df['hashtags'] = df['hashtags'].fillna('').astype(str)\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "df['captions'] = df['captions'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['captions'] = df['captions'].apply(lambda x: x.lower())\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: x.lower())\n",
    "\n",
    "# # Load the Excel file into a pandas dataframe\n",
    "# df = pd.read_excel('testingset.xlsx')\n",
    "# #print(df)\n",
    "# # Remove irrelevant columns\n",
    "# df = df[['username', 'captions', 'hashtags']]\n",
    "\n",
    "# # Handle missing values\n",
    "# #df.dropna(inplace=True)\n",
    "\n",
    "# # Remove punctuation and special characters\n",
    "# df['captions'] = df['captions'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "# df['hashtags'] = df['hashtags'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# # Convert text to lowercase\n",
    "# df['captions'] = df['captions'].apply(lambda x: x.lower())\n",
    "# df['hashtags'] = df['hashtags'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Lemmatize the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['captions'] = df['captions'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Create a corpus for each page separately\n",
    "corpora = []\n",
    "pages = df['username'].unique()\n",
    "#print(pages)\n",
    "for page in pages:\n",
    "    page_df = df[df['username'] == page]\n",
    "    page_corpus = page_df['captions'] + ' ' + page_df['hashtags']\n",
    "    corpora.append(page_corpus)\n",
    "#print(corpora)\n",
    "\n",
    "\n",
    "\n",
    "#**********************************************#\n",
    "# Get the list of column names from the subset\n",
    "#**********************************************#\n",
    "df = pd.read_excel('trainingset.xlsx')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "subset_df = df.iloc[:,1 :-1]\n",
    "#print(subset_df)\n",
    "column_names = subset_df.columns.tolist()\n",
    "\n",
    "#print(column_names)\n",
    "column_dict = {\"column_names\": column_names}\n",
    "# print(column_dict)\n",
    "# Print the column names for each username\n",
    "# for username, columns in column_dict.items():\n",
    "#     print(f\"Columns for username '{username}': {columns}\")\n",
    "#**********************************************#\n",
    "#Frequency of the Words Features present in Testing FIle #\n",
    "#**********************************************#\n",
    "\n",
    "#print(column_dict.items())\n",
    "\n",
    "column_frequency_dict={}\n",
    "\n",
    "for user,keywords in column_dict.items():\n",
    "    keyword_scores = {}\n",
    "    sum_keyword_scores = 0\n",
    "    for keyword in keywords:\n",
    "        keyword_score = 0\n",
    "        for i, page_corpus in enumerate(corpora):\n",
    "            # Calculate the frequency of the keyword in the page corpus\n",
    "            keyword_frequency = page_corpus.str.count(keyword).sum()\n",
    "            #print(pages)\n",
    "            #print(keyword_frequency)\n",
    "\n",
    "             # Add the keyword frequency to the keyword frequency dictionary\n",
    "            page_name = pages[i]\n",
    "            if page_name not in column_frequency_dict:\n",
    "                column_frequency_dict[page_name] = {}\n",
    "            column_frequency_dict[page_name][keyword] = keyword_frequency\n",
    "            #print(column_frequency_dict[page_name])\n",
    "\n",
    "\n",
    "#**********************************************#\n",
    "#Forming Testing File to Test the Model #\n",
    "#**********************************************#\n",
    "           \n",
    "# Get the list of unique usernames\n",
    "usernames = df['username'].unique()\n",
    "    \n",
    "\n",
    "# Convert the frequency dictionary into a DataFrame\n",
    "frequency_df = pd.DataFrame.from_dict(column_frequency_dict)\n",
    "\n",
    "# Transpose the DataFrame so that the usernames are in the rows and the keywords are in the columns\n",
    "frequency_df = frequency_df.transpose()\n",
    "\n",
    "# Add a column for the usernames\n",
    "frequency_df.insert(0, 'username', frequency_df.index)\n",
    "\n",
    "# Reset the index\n",
    "frequency_df = frequency_df.reset_index(drop=True)\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "frequency_df.to_excel('modeltesting.xlsx', index=False)\n",
    "print(\"\\033[1m Excel File Created for Testing the Model to pass in the Model to Test\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d97624",
   "metadata": {},
   "source": [
    "# Training the Model on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa346d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.joblib']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = pd.read_excel('trainingset.xlsx')\n",
    "\n",
    "# Split the data into input features and labels\n",
    "X = data.iloc[:, 1:-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Create a random forest classifier object\n",
    "rf = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "\n",
    "# Train the random forest model on the data\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf, 'random_forest_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f511fce",
   "metadata": {},
   "source": [
    "# Testing the Model on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "068c740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "# Load the model\n",
    "model = joblib.load('random_forest_model.joblib')\n",
    "\n",
    "# Load the testing data\n",
    "testing_df = pd.read_excel(\"modeltesting.xlsx\")\n",
    "\n",
    "# Use all columns except the first as input features\n",
    "features_df = testing_df.iloc[:, 1:]\n",
    "\n",
    "#print(features_df)\n",
    "# Predict the category for each row using the model\n",
    "predictions = model.predict(features_df)\n",
    "\n",
    "# Add the predicted category to the original testing dataframe\n",
    "testing_df[\"predicted_category\"] = predictions\n",
    "\n",
    "# Save the results to a new excel file\n",
    "testing_df.to_excel(\"randomforest_modeltesting_results.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3464456",
   "metadata": {},
   "source": [
    "#  Appending the Actual Category of the Pages of Testing File in the Random Forest Model Results File to get Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f5add21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original DataFrame from the file\n",
    "df = pd.read_excel('testingset.xlsx')\n",
    "\n",
    "# Get the last column (actual category) and the username column\n",
    "last_column = df.iloc[:, -1]\n",
    "username_column = df['username']\n",
    "\n",
    "# Create a dictionary to store unique username and corresponding actual category\n",
    "username_category_dict = {}\n",
    "\n",
    "# Iterate over the username and last column\n",
    "for username, category in zip(username_column, last_column):\n",
    "    if username not in username_category_dict:\n",
    "        # If the username is not already in the dictionary, add it with the corresponding category\n",
    "        username_category_dict[username] = category\n",
    "\n",
    "# Read the SVMresults.xlsx file\n",
    "result_df = pd.read_excel('randomforest_modeltesting_results.xlsx')\n",
    "\n",
    "\n",
    "# Create a new column to store the actual category based on unique username\n",
    "result_df['actualcategory'] = result_df['username'].map(username_category_dict)\n",
    "\n",
    "# Write the updated DataFrame back to the SVMresults.xlsx file\n",
    "result_df.to_excel('randomforest_modeltesting_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693598d",
   "metadata": {},
   "source": [
    "# Calculating Performance Metric of Model Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d1499c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 2 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 3]]\n",
      "\n",
      "Overall Accuracy: 0.9090909090909091\n",
      "Accuracy Percentage: 90.91%\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mfood\u001b[0m\n",
      "Precision: 1.0\n",
      "Recall: 0.75\n",
      "F1 Score: 0.8571428571428571\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mBeauty\u001b[0m\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mGiftShop\u001b[0m\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mFitness\u001b[0m\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n",
      "\u001b[1mCategory\u001b[0m: \u001b[1mClothing\u001b[0m\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "bold_start = \"\\033[1m\"\n",
    "bold_end = \"\\033[0m\"\n",
    "\n",
    "# Load the Excel sheet into a DataFrame\n",
    "df = pd.read_excel('randomforest_modeltesting_results.xlsx')\n",
    "\n",
    "#confusion matrix\n",
    "actual_categories = result_df['actualcategory']\n",
    "predicted_categories = result_df['predicted_category']\n",
    "conf_matrix = confusion_matrix(actual_categories, predicted_categories)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(actual_categories, predicted_categories)\n",
    "print(\"\\nOverall Accuracy:\", overall_accuracy)\n",
    "print(\"Accuracy Percentage: {:.2%}\".format(overall_accuracy))\n",
    "\n",
    "# Calculate metrics for each category\n",
    "categories = df['actualcategory'].unique()\n",
    "for category in categories:\n",
    "    # Filter the DataFrame for the current category\n",
    "    category_df = df[df['actualcategory'] == category]\n",
    "\n",
    "    # For Multi-Class Classification to get Precision, Recall, F1_score we have to set average='weighted'\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(y_true=category_df['actualcategory'], y_pred=category_df['predicted_category'], average='weighted')\n",
    "    recall = recall_score(category_df['actualcategory'], category_df['predicted_category'], average='weighted')\n",
    "    f1 = f1_score(category_df['actualcategory'], category_df['predicted_category'], average='weighted')\n",
    "\n",
    "\n",
    "    # Print the metrics for the current category\n",
    "    text = \"Category\"\n",
    "    print(bold_start + text + bold_end + \":\", bold_start + str(category) + bold_end)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
